{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e9fb5e-5cba-4327-a0fa-9f7c82fab45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n",
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "'''In min-max you will subtract the minimum value in the dataset with all the values and then divide this by the range of the\n",
    "   dataset(maximum-minimum). In this case, your dataset will lie between 0 and 1 in all cases whereas in the previous case, \n",
    "   it was between -1 and +1.\n",
    "   Example:\n",
    "    Linear Regression, Logistic Regression, KNN\n",
    "    '''\n",
    "\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = asarray([[100, 0.001],\n",
    " [8, 0.05],\n",
    " [50, 0.005],\n",
    " [88, 0.07],\n",
    " [4, 0.1]])\n",
    "print(data)\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5ae412-a158-4a13-8a85-a4f66349f590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite\\n   useful.\\n   Example: when dealing with image data, the colors can range from only 0 to 255.\\n   The MinMaxscaler is a type of scaler that scales the minimum and maximum values to be 0 and 1 respectively. While the \\n   StandardScaler scales all values between min and max so that they fall within a range from min to max.\\n   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''The Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite\n",
    "   useful.\n",
    "   Example: when dealing with image data, the colors can range from only 0 to 255.\n",
    "   The MinMaxscaler is a type of scaler that scales the minimum and maximum values to be 0 and 1 respectively. While the \n",
    "   StandardScaler scales all values between min and max so that they fall within a range from min to max.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb3ffe6-f8fe-491f-a260-8da429a6fa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "'''Introduction to Principal Component Analysis(PCA)\n",
    "   PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the\n",
    "   directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions\n",
    "   than the original one.\n",
    "   This is a technique that comes from the field of linear algebra and can be used as a data preparation technique to create \n",
    "   a projection of a dataset prior to fitting a model\n",
    "   Example:\n",
    "   '''\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7174bc-7747-4f09-97ef-75f4d959834d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PCA is an important method for feature extraction and image representation. In PCA, matrix transformation of the image \\n   takes place into high dimension vectors and its covariance matrix is obtained consuming high-dimension vector space.\\n   PCA reduces the dimensionality without losing information from any features. Speed up the learning algorithm\\n   (with lower dimension). Address the multicollinearity issue (all principal components are orthogonal to each other).\\n   Help visualize data with high dimensionality (after reducing the dimension to 2 or 3).\\n   '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''PCA is an important method for feature extraction and image representation. In PCA, matrix transformation of the image \n",
    "   takes place into high dimension vectors and its covariance matrix is obtained consuming high-dimension vector space.\n",
    "   PCA reduces the dimensionality without losing information from any features. Speed up the learning algorithm\n",
    "   (with lower dimension). Address the multicollinearity issue (all principal components are orthogonal to each other).\n",
    "   Help visualize data with high dimensionality (after reducing the dimension to 2 or 3).\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa317e7-6ff0-4c99-acb4-1570cbfb6385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Min-Max scaling is typically done via the following equation: Xsc=X−XminXmax−Xmin. One family of algorithms that is\\n   scale-invariant encompasses tree-based learning algorithms.\\n   Min-Max scaling is a normalization technique that enables us to scale data in a dataset to a specific range using each\\n   feature's minimum and maximum value.\\n   \\n   the MinMaxScaler and other scaling techniques is as follows:\\n1. Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the\\n   minimum and maximum observable values. This is done by calling the fit() function.\\n2. Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling \\n   the transform() function.\\n3. Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make \\n   predictions.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''A Min-Max scaling is typically done via the following equation: Xsc=X−XminXmax−Xmin. One family of algorithms that is\n",
    "   scale-invariant encompasses tree-based learning algorithms.\n",
    "   Min-Max scaling is a normalization technique that enables us to scale data in a dataset to a specific range using each\n",
    "   feature's minimum and maximum value.\n",
    "   \n",
    "   the MinMaxScaler and other scaling techniques is as follows:\n",
    "1. Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the\n",
    "   minimum and maximum observable values. This is done by calling the fit() function.\n",
    "2. Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling \n",
    "   the transform() function.\n",
    "3. Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make \n",
    "   predictions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a173ca-9d53-421d-be46-e7dbe7d96e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PCA generally tries to find the lower-dimensional surface to project the high-dimensional data. PCA works by considering \\n   the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces \\n   the dimensionality.\\n   Introduction to Principal Component Analysis\\n1. Standardize the d-dimensional dataset.\\n2. Construct the covariance matrix.\\n3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\\n4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''PCA generally tries to find the lower-dimensional surface to project the high-dimensional data. PCA works by considering \n",
    "   the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces \n",
    "   the dimensionality.\n",
    "   Introduction to Principal Component Analysis\n",
    "1. Standardize the d-dimensional dataset.\n",
    "2. Construct the covariance matrix.\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56e6545-e981-4203-8a3a-f3768ad037a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  5 10 15 20]\n",
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "# Q7\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = asarray([1, 5, 10, 15, 20])\n",
    "print(data)\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a489921-7b37-40f1-95ab-80f010f17f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PCA is a dimensionality reduction technique that has four main parts: feature covariance, eigendecomposition, principal\\n   component transformation, and choosing components in terms of explained variance. The purpose of this blog is to share a\\n   visual demo that helped the students understand the final two steps.\\nExample:\\nfrom sklearn.datasets import make_classification\\nX, y = make_classification(n_samples=135, n_weight=50, n_age=35, n_gender=2, blood_pressure=135)\\nprint(X.shape, y.shape)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8\n",
    "'''PCA is a dimensionality reduction technique that has four main parts: feature covariance, eigendecomposition, principal\n",
    "   component transformation, and choosing components in terms of explained variance. The purpose of this blog is to share a\n",
    "   visual demo that helped the students understand the final two steps.\n",
    "Example:\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=135, n_weight=50, n_age=35, n_gender=2, blood_pressure=135)\n",
    "print(X.shape, y.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3476fc-4b46-4747-b929-bcfa83ddbc83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
